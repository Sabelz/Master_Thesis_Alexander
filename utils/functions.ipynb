{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOB+CzlWjQ7xVaeScVkD+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabelz/Master_Thesis_Alexander/blob/main/utils/functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for the project"
      ],
      "metadata": {
        "id": "7PrRN4ckBB6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "EkZuAB9jBwQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Master_Thesis_Alexander\n",
        "!git config --global user.email \"alexander.sabelstrom.1040@student.uu.se\"\n",
        "!git config --global user.name \"Sabelz\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "!pip install gpytorch\n",
        "import gpytorch\n",
        "!pip install jaxopt\n",
        "import jaxopt\n",
        "import optax\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56clfMhBHm_",
        "outputId": "d3157e6c-7a62-47ba-d044-9001310405d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Master_Thesis_Alexander\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "PibOrFKwChDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, likelihood, x_train, y_train, training_iter=10):\n",
        "    \"\"\"\n",
        "    Trains a Gaussian Process (GP) model using the Adam optimizer and plots the training loss.\n",
        "\n",
        "    Parameters:\n",
        "    model (gpytorch.models.GP): The GP model.\n",
        "    likelihood (gpytorch.likelihoods.GaussianLikelihood): The likelihood function used in the GP model.\n",
        "    x_train (torch.Tensor): The training input data.\n",
        "    y_train (torch.Tensor): The training output data.\n",
        "    training_iter (int, optional): The number of training iterations. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "    time (float): The time taken to train the model.\n",
        "\n",
        "    Note:\n",
        "    The function uses the torch library to perform the computations. If a GPU is available,\n",
        "    it moves the model and likelihood to the GPU before training. The training loss is plotted\n",
        "    against the iteration number.\n",
        "    \"\"\"\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      likelihood = likelihood.cuda()\n",
        "    model.train()\n",
        "    likelihood.train()\n",
        "    # Use the adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.2)  # Includes GaussianLikelihood parameters\n",
        "\n",
        "    # \"Loss\" for GPs - the marginal log likelihood\n",
        "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "    loss_list = [] # Keep track of all losses\n",
        "    # Time the training\n",
        "    start = time.time()\n",
        "    for i in range(training_iter):\n",
        "        # Zero gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        # Output from model\n",
        "        output = model(x_train)\n",
        "        # Calc loss and backprop gradients\n",
        "        loss = -mll(output, y_train)\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    # Plot the training loss\n",
        "    plt.plot(list(range(1, training_iter+1)), loss_list)\n",
        "    plt.xlabel(\"Training Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    return end - start\n"
      ],
      "metadata": {
        "id": "fGH9HHCGCjoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function for Variational GPs"
      ],
      "metadata": {
        "id": "I4s18iGu91tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ELBO(model, likelihood, x_train, y_train, training_iter=25, train_loader=None):\n",
        "    \"\"\"\n",
        "    Trains a Gaussian Process (GP) model using the Adam optimizer and plots the training loss.\n",
        "    The model is trained using the Variational Evidence Lower BOund (ELBO) as the loss function.\n",
        "\n",
        "    Parameters:\n",
        "    model (gpytorch.models.ApproximateGP): The GP model.\n",
        "    likelihood (gpytorch.likelihoods.Likelihood): The likelihood function used in the GP model.\n",
        "    x_train (torch.Tensor): The training input data.\n",
        "    y_train (torch.Tensor): The training output data.\n",
        "    training_iter (int, optional): The number of training iterations. Default is 25.\n",
        "    train_loader (torch.utils.data.DataLoader, optional): The DataLoader for batch training.\n",
        "     If None, the model is trained on the entire dataset at once. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    time (float): The time taken to train the model.\n",
        "\n",
        "    Note:\n",
        "    The function uses the torch library to perform the computations. If a GPU is available,\n",
        "    it moves the model and likelihood to the GPU before training. The training loss is plotted\n",
        "    against the iteration number.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      likelihood = likelihood.cuda()\n",
        "\n",
        "    model.train()\n",
        "    likelihood.train()\n",
        "    # Initialize MLL\n",
        "    n_points = y_train.numel() # Amount of training points\n",
        "    # When training a variational Gaussian Process (GP) model like ApproximateGP,\n",
        "    # you should use a variational marginal log likelihood (MLL) instead of the exact MLL.\n",
        "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, n_points) # Loss\n",
        "    # Use the adam optimizer\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.2)\n",
        "    loss_list = []\n",
        "    if(train_loader == None):\n",
        "      # Time the training\n",
        "      start = time.time()\n",
        "      for i in range(training_iter):\n",
        "          # Zero gradients from previous iteration\n",
        "          optimizer.zero_grad()\n",
        "          # Output from model\n",
        "          output = model(x_train)\n",
        "          # Calc loss and backprop gradients\n",
        "          loss = -mll(output, y_train)\n",
        "          print\n",
        "          loss_list.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "    else: # If train_loader defined, use it\n",
        "      # Time the training\n",
        "      start = time.time()\n",
        "      for i in range(training_iter):\n",
        "        for x_batch, y_batch in train_loader:\n",
        "          # Zero gradients from previous iteration\n",
        "          optimizer.zero_grad()\n",
        "          # Output from model\n",
        "          output = model(x_batch)\n",
        "          # Calc loss and backprop gradients\n",
        "          loss = -mll(output, y_batch)\n",
        "          loss_list.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    # Plot the training loss\n",
        "    plt.plot(list(range(1, len(loss_list)+1)), loss_list)\n",
        "    plt.xlabel(\"Training Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    return end - start"
      ],
      "metadata": {
        "id": "QUHDTM7_957k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Function"
      ],
      "metadata": {
        "id": "ucNcGiyBIrFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, likelihood, test_x):\n",
        "    \"\"\"\n",
        "    This function makes predictions using a given model and likelihood.\n",
        "\n",
        "    The function sets the model and likelihood to evaluation mode,\n",
        "    then computes the likelihood of the model's predictions on the test data.\n",
        "    It uses PyTorch's `no_grad` context manager to avoid tracking gradients during the prediction,\n",
        "    and GPyTorch's `fast_pred_var` setting for efficient computation.\n",
        "\n",
        "    Parameters:\n",
        "    model (gpytorch.models.GP): The Gaussian Process model to make predictions with.\n",
        "    likelihood (gpytorch.likelihoods.Likelihood): The likelihood associated with the model.\n",
        "    test_x (torch.Tensor): The test inputs to make predictions on.\n",
        "\n",
        "    Returns:\n",
        "    gpytorch.distributions.MultivariateNormal: The distribution of the model's predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    likelihood.eval()\n",
        "    # Make predictions by feeding model through likelihood\n",
        "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "        return likelihood(model(test_x))"
      ],
      "metadata": {
        "id": "WcjCXqaNIs8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Function GPyTorch"
      ],
      "metadata": {
        "id": "Zl5fIjXW-tGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotGP(x_train, y_train, model, likelihood, title=\"GP Model\"):\n",
        "    \"\"\"\n",
        "    Plots the Gaussian Process (GP) model predictions along with the observed data.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (torch.Tensor): The training input data.\n",
        "    y_train (torch.Tensor): The training output data.\n",
        "    model (gpytorch.models.GP): The GP model.\n",
        "    likelihood (gpytorch.likelihoods.GaussianLikelihood): The likelihood function used in the GP model.\n",
        "    title (str, optional): The title of the plot. Default is \"GP Model\".\n",
        "\n",
        "    Note:\n",
        "    The function uses matplotlib to create the plot. The plot includes the mean predictions,\n",
        "    a confidence region based on the variances, and the observed data. The x-axis limits are set\n",
        "    based on the minimum and maximum values of the training data.\n",
        "    \"\"\"\n",
        "    # Find min and max value of training set\n",
        "    min_value, max_value = min(x_train), max(x_train)\n",
        "    # Create points between min and max values\n",
        "    x_plot = torch.linspace(min_value, max_value, 1000)\n",
        "    model.eval(), likelihood.eval()\n",
        "    # Evaluate on plot values\n",
        "    prediction = likelihood(model(x_plot))\n",
        "    model.train(), likelihood.train()\n",
        "    mean = prediction.mean\n",
        "    variance = prediction.variance\n",
        "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "      # Initalize plot\n",
        "      plt.style.use('default')\n",
        "      _, ax = plt.subplots(1, 1)\n",
        "\n",
        "      # Confidence region\n",
        "      lower_bound = mean-(1.96*(np.sqrt(variance)))\n",
        "      upper_bound = mean+(1.96*(np.sqrt(variance)))\n",
        "\n",
        "      ax.plot(x_train.detach().numpy(), y_train.detach().numpy(), 'ko', label='Observed Data', alpha = 0.7)\n",
        "      # Plot predictive means\n",
        "      ax.plot(x_plot.detach().numpy(), mean.detach().numpy(), 'purple', label='Mean')\n",
        "      # Plot confidence bounds as lightly shaded region\n",
        "      ax.fill_between(x_plot.detach().numpy(), lower_bound.detach().numpy(),\n",
        "                      upper_bound.detach().numpy(), alpha=0.5, color=\"violet\", zorder=-1, label ='95% Confidence')\n",
        "      ax.set_title(title)\n",
        "      ax.legend(loc = \"best\")\n",
        "      plt.grid(False)\n",
        "      ax.plot"
      ],
      "metadata": {
        "id": "hHUNXmPq-vhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Function for State Space Model"
      ],
      "metadata": {
        "id": "_WgPoepdpl3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For plotting state space with different hyperparameters\n",
        "def plotGP_SS(x_train, y_train, ell=1, sigma=1, m0=0, v0=1, title=\"GP Model\", n_test_points=1000):\n",
        "    \"\"\"\n",
        "    Plots the State Space Gaussian Process (SSGP) model predictions along with the observed data.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (jax.numpy.ndarray): The training input data.\n",
        "    y_train (jax.numpy.ndarray): The training output data.\n",
        "    ell (float, optional): The length-scale parameter for the SSGP model. Default is 1.\n",
        "    sigma (float, optional): The signal variance parameter for the SSGP model. Default is 1.\n",
        "    m0 (float, optional): The initial mean for the Kalman filter. Default is 0.\n",
        "    v0 (float, optional): The initial variance for the Kalman filter. Default is 1.\n",
        "    title (str, optional): The title of the plot. Default is \"GP Model\".\n",
        "    n_test_points (int, optional): The number of test points to generate between the minimum and maximum values of the training data. Default is 1000.\n",
        "\n",
        "    Note:\n",
        "    The function uses matplotlib to create the plot. The plot includes the mean predictions,\n",
        "    a confidence region based on the variances, and the observed data. The x-axis limits are set\n",
        "    based on the minimum and maximum values of the training data.\n",
        "    \"\"\"\n",
        "    # Find min and max value of training set\n",
        "    min_value, max_value = min(x_train).numpy(), max(x_train).numpy()\n",
        "    # Create points between min and max values\n",
        "    x_test = np.linspace(min_value, max_value, 1000)\n",
        "\n",
        "    all_points = jnp.concatenate([x_train.numpy(), x_test])\n",
        "    temporal_order = jnp.argsort(all_points)\n",
        "    # State Space X's and Y's\n",
        "    ss_xs = all_points[temporal_order]\n",
        "    ss_ys = jnp.concatenate([y_train.numpy(), jnp.nan * jnp.ones((n_test_points, ))])[temporal_order]\n",
        "    # Compute the equivalent SS model\n",
        "    dts = jnp.diff(ss_xs, prepend=min_value.item())\n",
        "    mfs, vfs, mps, vps, _ = kalmanFilter(ss_ys, dts, ell, sigma, m0 = m0, v0=v0)\n",
        "    # Smoothed means and variances\n",
        "    mss, vss = kalmanSmoothing(ell, dts, mfs, vfs, mps, vps)\n",
        "\n",
        "    # Posterior distribution\n",
        "    ss_posterior_mean = mss[jnp.isnan(ss_ys)]\n",
        "    ss_posterior_var = vss[jnp.isnan(ss_ys)]\n",
        "\n",
        "    plt.style.use('default')\n",
        "    _, ax = plt.subplots(1, 1)\n",
        "\n",
        "    ax.scatter(x_train.detach().numpy(), y_train.detach().numpy(), color='k', marker='o', label='Observed data', alpha=0.7)\n",
        "    ax.plot(x_test, ss_posterior_mean, label=\"Mean\", color = 'purple', alpha = 1)\n",
        "    ax.fill_between(x_test,\n",
        "                        ss_posterior_mean - 1.96 * jnp.sqrt(ss_posterior_var),\n",
        "                        ss_posterior_mean + 1.96 * jnp.sqrt(ss_posterior_var),\n",
        "                        alpha=0.5,\n",
        "                      label=\"95% Confidence\", color = \"violet\", zorder=-1)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlim([min_value, max_value])\n",
        "    ax.legend(loc=\"best\")\n",
        "    plt.grid(False)\n",
        "    ax.plot"
      ],
      "metadata": {
        "id": "YEuWV06Fp1HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot State Space GP"
      ],
      "metadata": {
        "id": "cIhLQTgd1V9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_SSGP(x_train, y_train, x_test, means, variances, title= \"GP Model\"):\n",
        "    \"\"\"\n",
        "    Plots the State Space Gaussian Process (SSGP) model predictions along with the observed data.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (jax.numpy.ndarray): The training input data.\n",
        "    y_train (jax.numpy.ndarray): The training output data.\n",
        "    x_test (jax.numpy.ndarray): The test input data.\n",
        "    means (jax.numpy.ndarray): The mean predictions of the SSGP model.\n",
        "    variances (jax.numpy.ndarray): The variance predictions of the SSGP model.\n",
        "    title (str, optional): The title of the plot. Default is \"GP Model\".\n",
        "\n",
        "    Note:\n",
        "    The function uses matplotlib to create the plot. The plot includes the mean predictions,\n",
        "    a confidence region based on the variances, and the observed data. The x-axis limits are set\n",
        "    based on the minimum and maximum values of the training data.\n",
        "    \"\"\"\n",
        "    # Find min and max value of training set\n",
        "    min_value, max_value = min(x_train).numpy(), max(x_train).numpy()\n",
        "    temporal_order = jnp.argsort(x_test.numpy())\n",
        "\n",
        "    # State Space X's and Y's\n",
        "    x_test_order = x_test.numpy()[temporal_order]\n",
        "    # Assuming means and variances already ordered\n",
        "    plt.style.use('default')\n",
        "    _, ax = plt.subplots(1, 1)\n",
        "    ax.plot(x_test_order, means, label = \"Mean\", color = \"purple\")\n",
        "    ax.scatter(x_train, y_train, label='Observed data', color = \"k\", marker=\"o\", alpha = 0.7)\n",
        "    ax.fill_between(x_test_order,\n",
        "                        means - 1.96 * jnp.sqrt(variances),\n",
        "                        means + 1.96 * jnp.sqrt(variances),\n",
        "                        alpha=0.3,\n",
        "                        label = \"Confidence Region\", color=\"violet\", zorder=-1)\n",
        "    ax.set_title(title)\n",
        "    #ax.set_xlim([min_value, max_value])\n",
        "    ax.legend(loc=\"best\")\n",
        "    plt.grid(False)"
      ],
      "metadata": {
        "id": "yZJDqD8r1Z1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filter and Smoother"
      ],
      "metadata": {
        "id": "hEWGoryjYDA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kalmanFilter(ss_ys, dts, ell, sigma, m0=0, v0=1, observation_cov=1):\n",
        "    \"\"\"\n",
        "    Implements the Kalman Filter algorithm for a given set of observations.\n",
        "\n",
        "    The function consists of two nested functions: `update` and `scan_body`.\n",
        "    The `update` function is responsible for updating the mean and variance\n",
        "    based on the observation and the observation covariance. The `scan_body`\n",
        "    function is used to scan through the observations and update the mean and\n",
        "    variance accordingly.\n",
        "\n",
        "    The function returns four arrays: `mfs`, `vfs`, `mps`, and `vps` which represent\n",
        "    the filtered means, filtered variances, predicted means, and predicted variances\n",
        "    respectively.\n",
        "\n",
        "    Note: This function uses the `jax.lax.scan` function for efficient looping over\n",
        "    the observations, and `jax.lax.cond` for conditionally updating the mean and\n",
        "    variance based on whether the observation is NaN.\n",
        "\n",
        "    Args:\n",
        "        m0 (float, optional): Initial mean for the Kalman filter. Defaults to 0.\n",
        "        v0 (float, optional): Initial variance for the Kalman filter. Defaults to 1.\n",
        "        ss_ys (array): Observations in the state space model.\n",
        "        Fs (array): Array of transition matrices.\n",
        "        Ws (array): Process noise covariance in the state space model.\n",
        "        observation_cov (float, optional): Observation covariance. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        mfs (array): Filtered means\n",
        "        vfs (array): Filtered variances\n",
        "        mps (array): Predicted means\n",
        "        vps (array): Predicted variances\n",
        "    \"\"\"\n",
        "    Fs = jnp.exp(-1 / ell * dts)\n",
        "    Ws = sigma ** 2 * (1 - jnp.exp(-2 / ell * dts))\n",
        "    def update(y, mp, vp):\n",
        "        S = vp + observation_cov\n",
        "        K = vp / S\n",
        "        v = y - mp\n",
        "        mf = mp + K * v\n",
        "        vf = vp - K * K * S\n",
        "        return mf, vf, -jax.scipy.stats.norm.logpdf(y, mp, jnp.sqrt(S))\n",
        "\n",
        "    def scan_body(carry, elem):\n",
        "        mf, vf, nll = carry\n",
        "        y, F, W = elem\n",
        "\n",
        "        mp = F * mf\n",
        "        vp = F * vf * F + W\n",
        "\n",
        "        mf, vf, nll_inc = jax.lax.cond(jnp.isnan(y),\n",
        "                                        lambda _: (mp, vp, 0.),\n",
        "                                        lambda _: update(y, mp, vp),\n",
        "                                        None)\n",
        "        nll = nll + nll_inc\n",
        "\n",
        "        return (mf, vf, nll), (mf, vf, mp, vp)\n",
        "    (_, _, nll), (mfs, vfs, mps, vps) = jax.lax.scan(scan_body, (m0, v0, 0.), (ss_ys, Fs, Ws))\n",
        "    return mfs, vfs, mps, vps, nll\n",
        "\n",
        "\n",
        "def kalmanSmoothing(ell, dts, mfs, vfs, mps, vps):\n",
        "    \"\"\"\n",
        "    Implements the Kalman Smoothing algorithm for a given set of filtered means and variances.\n",
        "\n",
        "    The function consists of a nested function: `scan_body`. The `scan_body` function is used to\n",
        "    scan through the filtered means and variances and update the smoothed means and variances accordingly.\n",
        "\n",
        "    The function returns two arrays: `mss` and `vss` which represent the smoothed means and smoothed variances respectively.\n",
        "\n",
        "    Note: This function uses the `jax.lax.scan` function for efficient looping over the filtered means and variances.\n",
        "\n",
        "    Args:\n",
        "        Fs (array): Array of transition matrices\n",
        "        mfs (array): Filtered means\n",
        "        vfs (array): Filtered variances\n",
        "        mps (array): Predicted means\n",
        "        vps (array): Predicted variances\n",
        "\n",
        "    Returns:\n",
        "        mss (array): Smoothed means\n",
        "        vss (array): Smoothed variances\n",
        "    \"\"\"\n",
        "    Fs = jnp.exp(-1 / ell * dts)\n",
        "    def scan_body(carry, elem):\n",
        "        ms, vs = carry\n",
        "        mf, vf, mp, vp, F = elem\n",
        "\n",
        "        G = vf * F / vp\n",
        "        ms = mf + G * (ms - mp)\n",
        "        vs = vf + G * (vs - vp) * G\n",
        "        return (ms, vs), (ms, vs)\n",
        "\n",
        "    _, smoothing_results = jax.lax.scan(scan_body,\n",
        "                                        (mfs[-1], vfs[-1]),\n",
        "                                        (mfs[:-1], vfs[:-1], mps[1:], vps[1:], Fs[1:]),\n",
        "                                        reverse=True)\n",
        "    mss = jnp.concatenate([smoothing_results[0], mfs[-1, None]], axis=0)\n",
        "    vss = jnp.concatenate([smoothing_results[1], vfs[-1, None]], axis=0)\n",
        "    return (mss, vss)"
      ],
      "metadata": {
        "id": "lxYa7N_oYEp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filter with PyTorch instead"
      ],
      "metadata": {
        "id": "CffafILR-Vgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kalmanFilter_torch(ss_ys, dts, ell, sigma, m0=0, v0=1, observation_cov=1):\n",
        "    \"\"\"\n",
        "    Implements the Kalman filter algorithm for a State Space Gaussian Process (SSGP) model using PyTorch.\n",
        "\n",
        "    Parameters:\n",
        "    ss_ys (torch.Tensor): The state space output data.\n",
        "    dts (torch.Tensor): The time differences between consecutive state space points.\n",
        "    ell (float): The length-scale parameter for the SSGP model.\n",
        "    sigma (float): The signal variance parameter for the SSGP model.\n",
        "    m0 (float, optional): The initial mean for the Kalman filter. Default is 0.\n",
        "    v0 (float, optional): The initial variance for the Kalman filter. Default is 1.\n",
        "    observation_cov (float, optional): The observation covariance for the Kalman filter. Default is 1.\n",
        "\n",
        "    Returns:\n",
        "    mfs (torch.Tensor): The filtered means.\n",
        "    vfs (torch.Tensor): The filtered variances.\n",
        "    mps (torch.Tensor): The predicted means.\n",
        "    vps (torch.Tensor): The predicted variances.\n",
        "    nll (float): The negative log likelihood.\n",
        "\n",
        "    Note:\n",
        "    The function uses the torch library to perform the computations.\n",
        "    \"\"\"\n",
        "    Fs = torch.exp(-1 / ell * dts)\n",
        "    Ws = sigma ** 2 * (1 - torch.exp(-2 / ell * dts))\n",
        "\n",
        "    def update(y, mp, vp):\n",
        "        S = vp + observation_cov\n",
        "        K = vp / S\n",
        "        v = y - mp\n",
        "        mf = mp + K * v\n",
        "        vf = vp - K * K * S\n",
        "        nll = -torch.distributions.Normal(mp, torch.sqrt(S)).log_prob(y)\n",
        "        return mf, vf, nll\n",
        "\n",
        "    mfs, vfs, nlls = [], [], []\n",
        "    mf, vf, nll = m0, v0, 0.\n",
        "    for y, F, W in zip(ss_ys, Fs, Ws):\n",
        "        mp = F * mf\n",
        "        vp = F * vf * F + W\n",
        "        if torch.isnan(y):\n",
        "            mf, vf, nll_inc = mp, vp, 0.\n",
        "        else:\n",
        "            mf, vf, nll_inc = update(y, mp, vp)\n",
        "        nll += nll_inc\n",
        "        mfs.append(mf)\n",
        "        vfs.append(vf)\n",
        "        nlls.append(torch.tensor(nll, requires_grad=True))\n",
        "\n",
        "    mfs = torch.stack(mfs)\n",
        "    vfs = torch.stack(vfs)\n",
        "    mps = Fs * mfs\n",
        "    vps = Fs * vfs * Fs + Ws\n",
        "    nll = torch.sum(torch.stack(nlls))\n",
        "\n",
        "    return mfs, vfs, mps, vps, nll"
      ],
      "metadata": {
        "id": "QsL_KEE0-YfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train function for State Space GPs"
      ],
      "metadata": {
        "id": "0fmlcuLR7ol2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SSGP(x_train, y_train, x_test, ell=1, sigma=1, training_iterations=25):\n",
        "    \"\"\"\n",
        "    Trains a State Space Gaussian Process (SSGP) model using JAX and the Adam optimizer.\n",
        "\n",
        "    This function takes training and test data as input, along with initial guesses for the\n",
        "    length scale (ell) and signal variance (sigma) hyperparameters of the Gaussian Process.\n",
        "    It performs optimization to minimize the negative log-likelihood (NLL) of the model\n",
        "    using the Adam optimizer from the optax library. The function also handles the conversion\n",
        "    of PyTorch tensors to NumPy arrays for compatibility with JAX.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (torch.Tensor): The input features for training, as a PyTorch tensor.\n",
        "    y_train (torch.Tensor): The target values for training, as a PyTorch tensor.\n",
        "    x_test (torch.Tensor): The input features for testing, as a PyTorch tensor.\n",
        "    ell (float, optional): The initial guess for the length scale hyperparameter. Default is 1.\n",
        "    sigma (float, optional): The initial guess for the signal variance hyperparameter. Default is 1.\n",
        "    training_iterations (int, optional): The number of iterations for the optimizer to run. Default is 25.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized length scale (ell), signal variance (sigma),\n",
        "           and the time taken for training (in seconds).\n",
        "\n",
        "    Note:\n",
        "    The function internally uses a bijection to ensure the positivity of the parameters during optimization.\n",
        "    It also uses a Kalman filter implementation (`kalmanFilter`) for the NLL calculation,\n",
        "    which should be defined elsewhere in the code. The inputs are expected to be PyTorch tensors,\n",
        "    which are converted to NumPy arrays for processing with JAX.\n",
        "    \"\"\"\n",
        "    n_test_points = len(x_test)\n",
        "    all_points = jnp.concatenate([x_train.numpy(), x_test.numpy()])\n",
        "    temporal_order = jnp.argsort(all_points)\n",
        "\n",
        "    # State Space X's and Y's\n",
        "    ss_xs = all_points[temporal_order]\n",
        "    ss_ys = jnp.concatenate([y_train.numpy(), jnp.nan * jnp.ones((n_test_points, ))])[temporal_order]\n",
        "    t0 = min(x_train).numpy().item()\n",
        "    dts = jnp.diff(ss_xs, prepend=t0)\n",
        "\n",
        "    # Bijection for the positivity of the parameters in the optimisation\n",
        "    def bijection(x):\n",
        "        return jnp.exp(x)\n",
        "\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Unpack parameters\n",
        "        ell, sigma = bijection(params)\n",
        "        *_, nll = kalmanFilter(ss_ys, dts, ell, sigma)\n",
        "        return nll\n",
        "\n",
        "    init_params = jnp.array(jnp.array([1., 1.]))  # Initial parameters\n",
        "    # Create an instance of the Adam optimizer from optax\n",
        "    solver = optax.adam(learning_rate=0.2)\n",
        "    opt_state = solver.init(init_params)\n",
        "    # Run optimization\n",
        "    start_time = time.time() # Time it\n",
        "    for _ in range(training_iterations):\n",
        "      grad = jax.grad(loss_fn)(init_params)\n",
        "      updates, opt_state = solver.update(grad, opt_state, init_params)\n",
        "      init_params = optax.apply_updates(init_params, updates)\n",
        "    end_time = time.time()\n",
        "    opt_ell, opt_sigma = bijection(init_params)\n",
        "\n",
        "\n",
        "    return opt_ell, opt_sigma, end_time-start_time"
      ],
      "metadata": {
        "id": "gvFDWZ187tl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train function for State Space GPs in pytorch"
      ],
      "metadata": {
        "id": "wyuMrLTcvPVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SSGP_torch(x_train, y_train, x_test, ell=1, sigma=1, training_iterations=25):\n",
        "    \"\"\"\n",
        "    Trains a State Space Gaussian Process (SSGP) model using PyTorch and the Adam optimizer.\n",
        "\n",
        "    This function takes training and test data as input, along with initial guesses for the\n",
        "    length scale (ell) and signal variance (sigma) hyperparameters of the Gaussian Process.\n",
        "    It performs optimization to minimize the negative log-likelihood (NLL) of the model\n",
        "    using the Adam optimizer. The function also handles CUDA availability for GPU acceleration.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (torch.Tensor): The input features for training.\n",
        "    y_train (torch.Tensor): The target values for training.\n",
        "    x_test (torch.Tensor): The input features for testing.\n",
        "    ell (float, optional): The initial guess for the length scale hyperparameter. Default is 1.\n",
        "    sigma (float, optional): The initial guess for the signal variance hyperparameter. Default is 1.\n",
        "    training_iterations (int, optional): The number of iterations for the optimizer to run. Default is 25.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized length scale (ell), signal variance (sigma),\n",
        "           and the time taken for training (in seconds).\n",
        "\n",
        "    Note:\n",
        "    The function internally uses a bijection to ensure the positivity of the parameters during optimization.\n",
        "    It also uses a Kalman filter implementation (`kalmanFilter_torch`) for the NLL calculation,\n",
        "    which should be defined elsewhere in the code.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      x_train = x_train.cuda()\n",
        "      y_train = y_train.cuda()\n",
        "      x_test = x_test.cuda()\n",
        "    n_test_points = len(x_test)\n",
        "    all_points = torch.cat([x_train, x_test])\n",
        "    temporal_order = torch.argsort(all_points)\n",
        "\n",
        "    # State Space X's and Y's\n",
        "    ss_xs = all_points[temporal_order]\n",
        "    ss_ys = torch.cat([y_train, torch.tensor([float('nan')] * n_test_points)])[temporal_order]\n",
        "    t0 = torch.tensor([min(x_train).item()])\n",
        "    if torch.cuda.is_available():\n",
        "      t0 = t0.cuda()\n",
        "    dts = torch.diff(ss_xs, prepend=t0)\n",
        "\n",
        "    # Bijection for the positivity of the parameters in the optimisation\n",
        "    def bijection(x):\n",
        "        return torch.exp(x)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Unpack parameters\n",
        "        ell, sigma = bijection(params)\n",
        "        *_, nll = kalmanFilter_torch(ss_ys, dts, ell, sigma)\n",
        "        return nll\n",
        "\n",
        "    # Run optimisation\n",
        "    init_params = torch.log(torch.tensor([1., 1.]))  # Initial parameters\n",
        "    if torch.cuda.is_available():\n",
        "      init_params = init_params.cuda()\n",
        "    init_params = Variable(init_params, requires_grad=True)\n",
        "\n",
        "    def loss_step():\n",
        "        opt.zero_grad()\n",
        "        loss = loss_fn(init_params)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    opt = torch.optim.Adam([init_params], lr=0.1)\n",
        "\n",
        "    start_time = time.time() # Time it\n",
        "\n",
        "    # Run the optimizer for a specific number of iterations\n",
        "    for _ in range(training_iterations):\n",
        "        opt.step(loss_step)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    opt_ell, opt_sigma = bijection(init_params.data)\n",
        "\n",
        "    return opt_ell, opt_sigma, end_time-start_time\n"
      ],
      "metadata": {
        "id": "0xxxK8CqvUZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error metrics"
      ],
      "metadata": {
        "id": "aVprVAK-2xrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_metrics(x_test, y_test, predictions, variances, confidence_level=0.95, scaler=None):\n",
        "    \"\"\"\n",
        "    Compute various error metrics for the given test data and predictions.\n",
        "\n",
        "    Parameters:\n",
        "    x_test (torch.Tensor): The test input data.\n",
        "    y_test (torch.Tensor): The true output data for the test set.\n",
        "    predictions (torch.Tensor): The model's predictions for the test data.\n",
        "    variances (torch.Tensor): The variances of the model's predictions.\n",
        "    confidence_level (float, optional): The confidence level for the prediction interval. Default is 0.95.\n",
        "    scaler (StandardScaler, optional): A fitted StandardScaler object if the 'y_test' was scaled. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    RMSE (float): The Root Mean Square Error between the true output and the predictions.\n",
        "    NLPD (float): The Negative Log Predictive Density.\n",
        "    PICP (float): The Prediction Interval Coverage Probability.\n",
        "    MPIW (float): The Mean Prediction Interval Width.\n",
        "\n",
        "    Note:\n",
        "    The function assumes that the input tensors are on a GPU and moves them to CPU.\n",
        "    \"\"\"\n",
        "    x_test_cpu = x_test.cpu()\n",
        "    y_test_cpu = y_test.cpu()\n",
        "    predictions_cpu = predictions.cpu()\n",
        "    variances_cpu = variances.cpu()\n",
        "    # Root Mean Square Error (RMSE) ---\n",
        "    if scaler is not None:\n",
        "      # Assuming 'y_test' was scaled with 'scaler'\n",
        "      y = scaler.inverse_transform(y_test_cpu)\n",
        "      p = scaler.inverse_transform(predictions_cpu)\n",
        "    else:\n",
        "      y = y_test_cpu\n",
        "      p = predictions_cpu\n",
        "    RMSE = mean_squared_error(y, p, squared=False)\n",
        "\n",
        "    # Negative Log Predictive Density (NLPD) ---\n",
        "\n",
        "    NLPD = torch.mean(0.5 * (np.log(2 * np.pi * variances_cpu))\n",
        "     + ((predictions_cpu - y_test_cpu) ** 2 / (variances_cpu*2))).item()\n",
        "    # Prediction Interval Coverage Probability (PICP) ---\n",
        "    z = norm.ppf(1-(1-confidence_level)/2)\n",
        "    # Confidence interval\n",
        "    lower_bound = (predictions_cpu - z*np.sqrt(variances_cpu))\n",
        "    upper_bound = (predictions_cpu + z*np.sqrt(variances_cpu))\n",
        "    PICP = np.mean((y_test_cpu.numpy() >= lower_bound.numpy()) & (y_test_cpu.numpy() <= upper_bound.numpy()))\n",
        "\n",
        "    # Mean Prediction Interval Width (MPIW) ---\n",
        "    MPIW = np.mean(2 * z * np.sqrt(variances_cpu.numpy()))\n",
        "\n",
        "    return RMSE, NLPD, PICP, MPIW"
      ],
      "metadata": {
        "id": "u2ZKsUfo208I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bar Plot Function"
      ],
      "metadata": {
        "id": "fwsVYAblJ6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bar(names, values, title, xlabel, ylabel):\n",
        "    \"\"\"\n",
        "    Creates a bar plot with the given data and labels.\n",
        "\n",
        "    Parameters:\n",
        "    names (list): A list of names for the bars. Must be the same length as 'values'.\n",
        "    values (list): A list of values for the bars. Must be the same length as 'names'.\n",
        "    title (str): The title of the plot.\n",
        "    xlabel (str): The label for the x-axis.\n",
        "    ylabel (str): The label for the y-axis.\n",
        "\n",
        "    Raises:\n",
        "    ValueError: If 'names' and 'values' are not the same length.\n",
        "\n",
        "    Note:\n",
        "    The function uses matplotlib to create the plot. The plot is displayed immediately.\n",
        "    \"\"\"\n",
        "    # Check if the lengths of times and models are equal\n",
        "    if len(names) != len(values):\n",
        "        raise ValueError(\"The lengths of names and values must be equal.\")\n",
        "\n",
        "\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(names, values, color='purple')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "beE840EVJ7ty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}