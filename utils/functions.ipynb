{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj2lrKnNh9T+FPvl+YQ0SB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabelz/Master_Thesis_Alexander/blob/main/utils/functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for the project"
      ],
      "metadata": {
        "id": "7PrRN4ckBB6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "EkZuAB9jBwQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Master_Thesis_Alexander\n",
        "!git config --global user.email \"alexander.sabelstrom.1040@student.uu.se\"\n",
        "!git config --global user.name \"Sabelz\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "!pip install gpytorch > \\dev\\null # Suppress prints\n",
        "import gpytorch\n",
        "!pip install jaxopt\n",
        "import jaxopt\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56clfMhBHm_",
        "outputId": "d3157e6c-7a62-47ba-d044-9001310405d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Master_Thesis_Alexander\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "PibOrFKwChDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, likelihood, x_train, y_train, training_iter=10):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      likelihood = likelihood.cuda()\n",
        "    model.train()\n",
        "    likelihood.train()\n",
        "    # Use the adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
        "\n",
        "    # \"Loss\" for GPs - the marginal log likelihood\n",
        "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "    loss_list = [] # Keep track of all losses\n",
        "    # Time the training\n",
        "    start = time.time()\n",
        "    for i in range(training_iter):\n",
        "        # Zero gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        # Output from model\n",
        "        output = model(x_train)\n",
        "        # Calc loss and backprop gradients\n",
        "        loss = -mll(output, y_train)\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    # Plot the training loss\n",
        "    plt.plot(list(range(1, training_iter+1)), loss_list)\n",
        "    plt.xlabel(\"Training Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    return end - start\n"
      ],
      "metadata": {
        "id": "fGH9HHCGCjoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function for Inducing Points GP"
      ],
      "metadata": {
        "id": "I4s18iGu91tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ELBO(model, likelihood, x_train, y_train, training_iter=10, train_loader=None):\n",
        "    \"\"\"\n",
        "    Trains a Gaussian Process model using the VariationalELBO loss function\n",
        "    with the option for early stopping based on a relative loss threshold.\n",
        "\n",
        "    Parameters:\n",
        "    model (gpytorch.models.ApproximateGP): The Gaussian Process model to be trained.\n",
        "    likelihood (gpytorch.likelihoods.Likelihood): The likelihood function to be used with the model.\n",
        "    x_train (torch.Tensor): The training data features.\n",
        "    y_train (torch.Tensor): The training data labels.\n",
        "    training_iter (int, optional): The number of training iterations. Default is 10.\n",
        "    threshold (float, optional): The relative loss improvement threshold for early stopping.\n",
        "              If the relative improvement in loss is less than this threshold, training is stopped.\n",
        "              If None, early stopping is not used. Default is None.\n",
        "    train_loader (torch.utils.data.DataLoader, optional): The DataLoader for batch training.\n",
        "              If None, the whole dataset is used for each training iteration. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    float: The total training time in seconds.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      likelihood = likelihood.cuda()\n",
        "    # Parameters and and input data should be of same dtype\n",
        "    model = model.double()\n",
        "    likelihood = likelihood.double()\n",
        "\n",
        "    model.train()\n",
        "    likelihood.train()\n",
        "    # Initialize MLL\n",
        "    n_points = y_train.numel() # Amount of training points\n",
        "    # Yes, when training a variational Gaussian Process (GP) model like ApproximateGP,\n",
        "    # you should use a variational marginal log likelihood (MLL) instead of the exact MLL.\n",
        "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, n_points) # Loss\n",
        "    # Use the adam optimizer\n",
        "    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
        "    loss_list = []\n",
        "    if(train_loader == None):\n",
        "      # Time the training\n",
        "      start = time.time()\n",
        "      for i in range(training_iter):\n",
        "          # Zero gradients from previous iteration\n",
        "          optimizer.zero_grad()\n",
        "          # Output from model\n",
        "          output = model(x_train)\n",
        "          # Calc loss and backprop gradients\n",
        "          loss = -mll(output, y_train)\n",
        "          loss_list.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "    else: # If train_loader defined, use it\n",
        "      # Time the training\n",
        "      start = time.time()\n",
        "      for i in range(training_iter):\n",
        "        for x_batch, y_batch in train_loader:\n",
        "          # Zero gradients from previous iteration\n",
        "          optimizer.zero_grad()\n",
        "          # Output from model\n",
        "          output = model(x_batch)\n",
        "          # Calc loss and backprop gradients\n",
        "          loss = -mll(output, y_batch)\n",
        "          loss_list.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    # Plot the training loss\n",
        "    plt.plot(list(range(1, len(loss_list)+1)), loss_list)\n",
        "    plt.xlabel(\"Training Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    return end - start"
      ],
      "metadata": {
        "id": "QUHDTM7_957k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Function"
      ],
      "metadata": {
        "id": "ucNcGiyBIrFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, likelihood, test_x):\n",
        "    \"\"\"\n",
        "    This function makes predictions using a given model and likelihood.\n",
        "\n",
        "    The function sets the model and likelihood to evaluation mode,\n",
        "    then computes the likelihood of the model's predictions on the test data.\n",
        "    It uses PyTorch's `no_grad` context manager to avoid tracking gradients during the prediction,\n",
        "    and GPyTorch's `fast_pred_var` setting for efficient computation.\n",
        "\n",
        "    Parameters:\n",
        "    model (gpytorch.models.GP): The Gaussian Process model to make predictions with.\n",
        "    likelihood (gpytorch.likelihoods.Likelihood): The likelihood associated with the model.\n",
        "    test_x (torch.Tensor): The test inputs to make predictions on.\n",
        "\n",
        "    Returns:\n",
        "    gpytorch.distributions.MultivariateNormal: The distribution of the model's predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    likelihood.eval()\n",
        "    # Make predictions by feeding model through likelihood\n",
        "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "        return likelihood(model(test_x))"
      ],
      "metadata": {
        "id": "WcjCXqaNIs8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Function GPyTorch"
      ],
      "metadata": {
        "id": "Zl5fIjXW-tGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotGP(x_train, y_train, model, likelihood, title=\"GP Model\"):\n",
        "  \"\"\"\n",
        "  This function plots the Gaussian Process regression model along with the observed data.\n",
        "\n",
        "  Parameters:\n",
        "  x_train (torch.Tensor): The training inputs.\n",
        "  y_train (torch.Tensor): The training targets.\n",
        "  model (gpytorch.models.GP): The Gaussian Process regression model.\n",
        "  likelihood (gpytorch.likelihoods.Likelihood): The likelihood function to use for the model.\n",
        "  title (str, optional): The title of the plot. Defaults to \"GP Model\".\n",
        "\n",
        "  Returns:\n",
        "  None. The function creates a plot and does not return anything.\n",
        "\n",
        "  \"\"\"\n",
        "  # Find min and max value of training set\n",
        "  min_value, max_value = min(x_train), max(x_train)\n",
        "  # Create points between min and max values\n",
        "  x_plot = torch.linspace(min_value, max_value, 1000)\n",
        "  model.eval(), likelihood.eval()\n",
        "  # Evaluate on plot values\n",
        "  prediction = likelihood(model(x_plot))\n",
        "  model.train(), likelihood.train()\n",
        "  mean = prediction.mean\n",
        "  variance = prediction.variance\n",
        "  with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "    # Initalize plot\n",
        "    plt.style.use('default')\n",
        "    _, ax = plt.subplots(1, 1)\n",
        "\n",
        "    # Confidence region\n",
        "    lower_bound = mean-(1.96*(np.sqrt(variance)))\n",
        "    upper_bound = mean+(1.96*(np.sqrt(variance)))\n",
        "\n",
        "    ax.plot(x_train.detach().numpy(), y_train.detach().numpy(), 'ko', label='Observed Data')\n",
        "    # Plot predictive means\n",
        "    ax.plot(x_plot.detach().numpy(), mean.detach().numpy(), 'purple', label='Mean')\n",
        "    # Plot confidence bounds as lightly shaded region\n",
        "    ax.fill_between(x_plot.detach().numpy(), lower_bound.detach().numpy(),\n",
        "                    upper_bound.detach().numpy(), alpha=0.5, color=\"violet\", zorder=-1, label ='95% Confidence')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc = \"best\")\n",
        "    plt.grid(False)\n",
        "    ax.plot\n"
      ],
      "metadata": {
        "id": "hHUNXmPq-vhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Function for State Space Model"
      ],
      "metadata": {
        "id": "_WgPoepdpl3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For plotting state space with different hyperparameters\n",
        "def plotGP_SS(x_train, y_train, ell=1, sigma=1, m0=0, v0=1, title=\"GP Model\", n_test_points=1000):\n",
        "  # Find min and max value of training set\n",
        "  min_value, max_value = min(x_train).numpy(), max(x_train).numpy()\n",
        "  # Create points between min and max values\n",
        "  x_test = np.linspace(min_value, max_value, 1000)\n",
        "\n",
        "  all_points = jnp.concatenate([x_train.numpy(), x_test])\n",
        "  temporal_order = jnp.argsort(all_points)\n",
        "  # State Space X's and Y's\n",
        "  ss_xs = all_points[temporal_order]\n",
        "  ss_ys = jnp.concatenate([y_train.numpy(), jnp.nan * jnp.ones((n_test_points, ))])[temporal_order]\n",
        "  # Compute the equivalent SS model\n",
        "  dts = jnp.diff(ss_xs, prepend=min_value.item())\n",
        "  mfs, vfs, mps, vps, _ = kalmanFilter(ss_ys, dts, ell, sigma, m0 = m0, v0=v0)\n",
        "  # Smoothed means and variances\n",
        "  mss, vss = kalmanSmoothing(ell, dts, mfs, vfs, mps, vps)\n",
        "\n",
        "  # Posterior distribution\n",
        "  ss_posterior_mean = mss[jnp.isnan(ss_ys)]\n",
        "  ss_posterior_var = vss[jnp.isnan(ss_ys)]\n",
        "\n",
        "  plt.style.use('default')\n",
        "  _, ax = plt.subplots(1, 1)\n",
        "\n",
        "  ax.scatter(x_train.detach().numpy(), y_train.detach().numpy(), color='k', marker='o', label='Observed data')\n",
        "  ax.plot(x_test, ss_posterior_mean, label=\"Mean\", color = 'purple', alpha = 1)\n",
        "  ax.fill_between(x_test,\n",
        "                      ss_posterior_mean - 1.96 * jnp.sqrt(ss_posterior_var),\n",
        "                      ss_posterior_mean + 1.96 * jnp.sqrt(ss_posterior_var),\n",
        "                      alpha=0.5,\n",
        "                     label=\"95% Confidence\", color = \"violet\", zorder=-1)\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlim([min_value, max_value])\n",
        "  ax.legend(loc=\"best\")\n",
        "  plt.grid(False)\n",
        "  ax.plot"
      ],
      "metadata": {
        "id": "YEuWV06Fp1HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot State Space GP"
      ],
      "metadata": {
        "id": "cIhLQTgd1V9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_SSGP(x_train, y_train, x_test, means, variances, title= \"GP Model\"):\n",
        "  # Find min and max value of training set\n",
        "  min_value, max_value = min(x_train).numpy(), max(x_train).numpy()\n",
        "  temporal_order = jnp.argsort(x_test.numpy())\n",
        "\n",
        "  # State Space X's and Y's\n",
        "  x_test_order = x_test.numpy()[temporal_order]\n",
        "  # Assuming means and variances already ordered\n",
        "  plt.style.use('default')\n",
        "  _, ax = plt.subplots(1, 1)\n",
        "  ax.plot(x_test_order, means, label = \"Mean\", color = \"purple\")\n",
        "  ax.scatter(x_train, y_train, s=1, alpha=0.5, label='Observed data', color = \"k\", marker=\"o\")\n",
        "  ax.fill_between(x_test_order,\n",
        "                      means - 1.96 * jnp.sqrt(variances),\n",
        "                      means + 1.96 * jnp.sqrt(variances),\n",
        "                      alpha=0.3,\n",
        "                      label = \"Confidence Region\", color=\"violet\", zorder=-1)\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlim([min_value, max_value])\n",
        "  ax.legend(loc=\"best\")\n",
        "  plt.grid(False)\n"
      ],
      "metadata": {
        "id": "yZJDqD8r1Z1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filter and Smoother"
      ],
      "metadata": {
        "id": "hEWGoryjYDA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kalmanFilter(ss_ys, dts, ell, sigma, m0=0, v0=1, observation_cov=1):\n",
        "    \"\"\"\n",
        "    Implements the Kalman Filter algorithm for a given set of observations.\n",
        "\n",
        "    The function consists of two nested functions: `update` and `scan_body`.\n",
        "    The `update` function is responsible for updating the mean and variance\n",
        "    based on the observation and the observation covariance. The `scan_body`\n",
        "    function is used to scan through the observations and update the mean and\n",
        "    variance accordingly.\n",
        "\n",
        "    The function returns four arrays: `mfs`, `vfs`, `mps`, and `vps` which represent\n",
        "    the filtered means, filtered variances, predicted means, and predicted variances\n",
        "    respectively.\n",
        "\n",
        "    Note: This function uses the `jax.lax.scan` function for efficient looping over\n",
        "    the observations, and `jax.lax.cond` for conditionally updating the mean and\n",
        "    variance based on whether the observation is NaN.\n",
        "\n",
        "    Args:\n",
        "        m0 (float, optional): Initial mean for the Kalman filter. Defaults to 0.\n",
        "        v0 (float, optional): Initial variance for the Kalman filter. Defaults to 1.\n",
        "        ss_ys (array): Observations in the state space model.\n",
        "        Fs (array): Array of transition matrices.\n",
        "        Ws (array): Process noise covariance in the state space model.\n",
        "        observation_cov (float, optional): Observation covariance. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        mfs (array): Filtered means\n",
        "        vfs (array): Filtered variances\n",
        "        mps (array): Predicted means\n",
        "        vps (array): Predicted variances\n",
        "    \"\"\"\n",
        "    Fs = jnp.exp(-1 / ell * dts)\n",
        "    Ws = sigma ** 2 * (1 - jnp.exp(-2 / ell * dts))\n",
        "    def update(y, mp, vp):\n",
        "        S = vp + observation_cov\n",
        "        K = vp / S\n",
        "        v = y - mp\n",
        "        mf = mp + K * v\n",
        "        vf = vp - K * K * S\n",
        "        return mf, vf, -jax.scipy.stats.norm.logpdf(y, mp, jnp.sqrt(S))\n",
        "\n",
        "    def scan_body(carry, elem):\n",
        "        mf, vf, nll = carry\n",
        "        y, F, W = elem\n",
        "\n",
        "        mp = F * mf\n",
        "        vp = F * vf * F + W\n",
        "\n",
        "        mf, vf, nll_inc = jax.lax.cond(jnp.isnan(y),\n",
        "                                        lambda _: (mp, vp, 0.),\n",
        "                                        lambda _: update(y, mp, vp),\n",
        "                                        None)\n",
        "        nll = nll + nll_inc\n",
        "\n",
        "        return (mf, vf, nll), (mf, vf, mp, vp)\n",
        "    (_, _, nll), (mfs, vfs, mps, vps) = jax.lax.scan(scan_body, (m0, v0, 0.), (ss_ys, Fs, Ws))\n",
        "    return mfs, vfs, mps, vps, nll\n",
        "\n",
        "\n",
        "def kalmanSmoothing(ell, dts, mfs, vfs, mps, vps):\n",
        "    \"\"\"\n",
        "    Implements the Kalman Smoothing algorithm for a given set of filtered means and variances.\n",
        "\n",
        "    The function consists of a nested function: `scan_body`. The `scan_body` function is used to\n",
        "    scan through the filtered means and variances and update the smoothed means and variances accordingly.\n",
        "\n",
        "    The function returns two arrays: `mss` and `vss` which represent the smoothed means and smoothed variances respectively.\n",
        "\n",
        "    Note: This function uses the `jax.lax.scan` function for efficient looping over the filtered means and variances.\n",
        "\n",
        "    Args:\n",
        "        Fs (array): Array of transition matrices\n",
        "        mfs (array): Filtered means\n",
        "        vfs (array): Filtered variances\n",
        "        mps (array): Predicted means\n",
        "        vps (array): Predicted variances\n",
        "\n",
        "    Returns:\n",
        "        mss (array): Smoothed means\n",
        "        vss (array): Smoothed variances\n",
        "    \"\"\"\n",
        "    Fs = jnp.exp(-1 / ell * dts)\n",
        "    def scan_body(carry, elem):\n",
        "        ms, vs = carry\n",
        "        mf, vf, mp, vp, F = elem\n",
        "\n",
        "        G = vf * F / vp\n",
        "        ms = mf + G * (ms - mp)\n",
        "        vs = vf + G * (vs - vp) * G\n",
        "        return (ms, vs), (ms, vs)\n",
        "\n",
        "    _, smoothing_results = jax.lax.scan(scan_body,\n",
        "                                        (mfs[-1], vfs[-1]),\n",
        "                                        (mfs[:-1], vfs[:-1], mps[1:], vps[1:], Fs[1:]),\n",
        "                                        reverse=True)\n",
        "    mss = jnp.concatenate([smoothing_results[0], mfs[-1, None]], axis=0)\n",
        "    vss = jnp.concatenate([smoothing_results[1], vfs[-1, None]], axis=0)\n",
        "    return (mss, vss)"
      ],
      "metadata": {
        "id": "lxYa7N_oYEp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filter with PyTorch instead"
      ],
      "metadata": {
        "id": "CffafILR-Vgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kalmanFilter_torch(ss_ys, dts, ell, sigma, m0=0, v0=1, observation_cov=1):\n",
        "    Fs = torch.exp(-1 / ell * dts)\n",
        "    Ws = sigma ** 2 * (1 - torch.exp(-2 / ell * dts))\n",
        "\n",
        "    def update(y, mp, vp):\n",
        "        S = vp + observation_cov\n",
        "        K = vp / S\n",
        "        v = y - mp\n",
        "        mf = mp + K * v\n",
        "        vf = vp - K * K * S\n",
        "        nll = -torch.distributions.Normal(mp, torch.sqrt(S)).log_prob(y)\n",
        "        return mf, vf, nll\n",
        "\n",
        "    mfs, vfs, nlls = [], [], []\n",
        "    mf, vf, nll = m0, v0, 0.\n",
        "    for y, F, W in zip(ss_ys, Fs, Ws):\n",
        "        mp = F * mf\n",
        "        vp = F * vf * F + W\n",
        "        if torch.isnan(y):\n",
        "            mf, vf, nll_inc = mp, vp, 0.\n",
        "        else:\n",
        "            mf, vf, nll_inc = update(y, mp, vp)\n",
        "        nll += nll_inc\n",
        "        mfs.append(mf)\n",
        "        vfs.append(vf)\n",
        "        nlls.append(nll)\n",
        "\n",
        "    mfs = torch.stack(mfs)\n",
        "    vfs = torch.stack(vfs)\n",
        "    mps = Fs * mfs\n",
        "    vps = Fs * vfs * Fs + Ws\n",
        "    nll = torch.sum(torch.stack(nlls))\n",
        "\n",
        "    return mfs, vfs, mps, vps, nll"
      ],
      "metadata": {
        "id": "QsL_KEE0-YfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train function for State Space GPs"
      ],
      "metadata": {
        "id": "0fmlcuLR7ol2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SSGP(x_train, y_train, x_test, ell=1, sigma=1):\n",
        "  n_test_points = len(x_test)\n",
        "  all_points = jnp.concatenate([x_train.numpy(), x_test.numpy()])\n",
        "  temporal_order = jnp.argsort(all_points)\n",
        "\n",
        "  # State Space X's and Y's\n",
        "  ss_xs = all_points[temporal_order]\n",
        "  ss_ys = jnp.concatenate([y_train.numpy(), jnp.nan * jnp.ones((n_test_points, ))])[temporal_order]\n",
        "  t0 = min(x_train).numpy().item()\n",
        "  dts = jnp.diff(ss_xs, prepend=t0)\n",
        "\n",
        "  # Bijection for the positivity of the parameters in the optimisation\n",
        "  def bijection(x):\n",
        "      return jnp.exp(x)\n",
        "\n",
        "\n",
        "  def loss_fn(params):\n",
        "      # Unpack parameters\n",
        "      ell, sigma = bijection(params)\n",
        "      *_, nll = kalmanFilter(ss_ys, dts, ell, sigma)\n",
        "      return nll\n",
        "\n",
        "  # Run optimisation\n",
        "  init_params = jnp.log(jnp.array([1., 1.]))  # Initial parameters\n",
        "  opt_solver = jaxopt.ScipyMinimize(method='L-BFGS-B', jit=True, fun=loss_fn)\n",
        "  start_time = time.time() # Time it\n",
        "  opt_params, opt_state = opt_solver.run(init_params)\n",
        "  end_time = time.time()\n",
        "  opt_ell, opt_sigma = bijection(opt_params)\n",
        "\n",
        "  return opt_ell, opt_sigma, end_time-start_time"
      ],
      "metadata": {
        "id": "gvFDWZ187tl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train function for State Space GPs in pytorch"
      ],
      "metadata": {
        "id": "wyuMrLTcvPVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SSGP_torch(x_train, y_train, x_test, ell=1, sigma=1):\n",
        "  n_test_points = len(x_test)\n",
        "  all_points = torch.cat([x_train, x_test])\n",
        "  temporal_order = torch.argsort(all_points)\n",
        "\n",
        "  # State Space X's and Y's\n",
        "  ss_xs = all_points[temporal_order]\n",
        "  ss_ys = torch.cat([y_train, torch.tensor([float('nan')] * n_test_points)])[temporal_order]\n",
        "  t0 = torch.tensor([min(x_train).item()])\n",
        "  dts = torch.diff(ss_xs, prepend=t0)\n",
        "\n",
        "  # Bijection for the positivity of the parameters in the optimisation\n",
        "  def bijection(x):\n",
        "      return torch.exp(x)\n",
        "\n",
        "  def loss_fn(params):\n",
        "      # Unpack parameters\n",
        "      ell, sigma = bijection(params)\n",
        "      *_, nll = kalmanFilter_torch(ss_ys, dts, ell, sigma)\n",
        "      return nll\n",
        "\n",
        "  # Run optimisation\n",
        "  init_params = torch.log(torch.tensor([1., 1.]))  # Initial parameters\n",
        "  init_params = Variable(init_params, requires_grad=True)\n",
        "\n",
        "  def loss_step():\n",
        "      opt.zero_grad()\n",
        "      loss = loss_fn(init_params)\n",
        "      loss.backward()\n",
        "      return loss\n",
        "\n",
        "  opt = torch.optim.LBFGS([init_params], lr=1, max_iter=1, history_size=100, line_search_fn='strong_wolfe')\n",
        "\n",
        "  start_time = time.time() # Time it\n",
        "  opt.step(loss_step)\n",
        "  end_time = time.time()\n",
        "\n",
        "  opt_ell, opt_sigma = bijection(init_params.data)\n",
        "\n",
        "  return opt_ell, opt_sigma, end_time-start_time\n"
      ],
      "metadata": {
        "id": "0xxxK8CqvUZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error metrics"
      ],
      "metadata": {
        "id": "aVprVAK-2xrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_metrics(x_test, y_test, predictions, variances, confidence_level=0.95, scaler=None):\n",
        "\n",
        "    x_test_cpu = x_test.cpu()\n",
        "    y_test_cpu = y_test.cpu()\n",
        "    predictions_cpu = predictions.cpu()\n",
        "    variances_cpu = variances.cpu()\n",
        "    # Root Mean Square Error (RMSE) ---\n",
        "    if scaler is not None:\n",
        "      # Assuming 'y_test' was scaled with 'scaler'\n",
        "      y = scaler.inverse_transform(y_test_cpu)\n",
        "      p = scaler.inverse_transform(predictions_cpu)\n",
        "    else:\n",
        "      y = y_test_cpu\n",
        "      p = predictions_cpu\n",
        "    RMSE = mean_squared_error(y, p, squared=False)\n",
        "\n",
        "    # Negative Log Predictive Density (NLPD) ---\n",
        "\n",
        "    NLPD = torch.mean(0.5 * (np.log(2 * np.pi * variances))\n",
        "     + ((predictions - y_test) ** 2 / (variances*2)))\n",
        "    # Prediction Interval Coverage Probability (PICP) ---\n",
        "    z = norm.ppf(1-(1-confidence_level)/2)\n",
        "    # Confidence interval\n",
        "    lower_bound = (predictions_cpu - z*np.sqrt(variances_cpu))\n",
        "    upper_bound = (predictions_cpu + z*np.sqrt(variances_cpu))\n",
        "    PICP = np.mean((y_test_cpu.numpy() >= lower_bound.numpy()) & (y_test_cpu.numpy() <= upper_bound.numpy()))\n",
        "\n",
        "    # Mean Prediction Interval Width (MPIW) ---\n",
        "    MPIW = np.mean(2 * z * np.sqrt(variances_cpu.numpy()))\n",
        "\n",
        "    return RMSE, NLPD, PICP, MPIW"
      ],
      "metadata": {
        "id": "u2ZKsUfo208I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bar Plot Function"
      ],
      "metadata": {
        "id": "fwsVYAblJ6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bar(names, values, title, xlabel, ylabel):\n",
        "  # Check if the lengths of times and models are equal\n",
        "  if len(names) != len(values):\n",
        "      raise ValueError(\"The lengths of names and values must be equal.\")\n",
        "\n",
        "\n",
        "  # Create a bar plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.bar(names, values, color='purple')\n",
        "\n",
        "  # Add labels and title\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(title)\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "beE840EVJ7ty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}